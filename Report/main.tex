\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{url}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\title{Linguistic A-Maze-ment: Solving Textual Mazes with Pretrained Language Models}
\author{\IEEEauthorblockN{Gabriel Del Castillo\IEEEauthorrefmark{1}, Nate Webster\IEEEauthorrefmark{2}}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Colorado School of Mines}\\
Golden, CO, USA \\
\IEEEauthorrefmark{1}gdelcastillo@mines.edu,
\IEEEauthorrefmark{2}nwebster@mines.edu
}
}

\begin{document}

\maketitle

\begin{abstract}
  % To be completed before final submission  
\end{abstract}

\section{Introduction}

Large Language Models (LLMs) such as GPT-4 and LlaMa 3 are able to produce remarkably accurate results when faced with various natural language processing tasks~\cite{openai}. As the scale and reach of these models have expanded, so has the focus on developing benchmarks to measure the limits and capabilities of LLMs~\cite{srivastava, liang}. However, there has been little research examining how pretrained language models handle maze solving tasks, along with identifying possible common failure patterns. The ability to solve maze-like structures has long served as a way to evaluate the cognitive and algorithmic performance of various subjects, with research ranging from rodent navigation~\cite{tolman}, to spatial understanding in infants~\cite{piaget}, to path planning for robotic systems~\cite{stentz}.

Motivated by this, as well as the prospect of integrating LLMs into physical robotic agents capable of speech, we analyze the responses of four different language models after being prompted with one of several solvable maze configurations. We evaluate the validity and optimality of their answers, and measure if parameter count, model distribution type, or maze representation have an impact on task performance.

The overarching goal of our work was to determine whether publicly available language models have sufficient spatial awareness to correctly execute a given navigational task. In turn, this serves as a foundation for deploying LLMs in contexts where they need to interact with the physical world.

\subsection{Related Work}
Popular LLM benchmarks, including MMLU \cite{hendrycks}, BIG-Bench \cite{srivastava}, and HELM \cite{liang} provide evaluation frameworks across several language and logic-related tasks, yet fail to test for a model's spatial reasoning or path-finding abilities. Research focused on evaluating language models in embodied settings, such as SayCan \cite{ahn} and ReAct \cite{yao}, reflects the growing interest in testing LLM's capacity for interaction beyond pure language. In addition, recent advances in automatic and customizable maze generation have made it feasible to evaluate symbolic spatial reasoning in LLMs. The library presented in \cite{ivanitskiy} enables configurable creation and representation of maze structures, which is well-suited for benchmarking purposes and is utilized in this work.

\section{Methodology}

\section{Results}

\section{Conclusion}

\section{Acknowledgements}

\begin{thebibliography}{00}

\bibitem{openai}
OpenAI, ``GPT-4 Technical Report,'' \emph{arXiv preprint arXiv:2303.08774}, 2023. [Online]. Available: \url{https://arxiv.org/abs/2303.08774}
\bibitem{srivastava}
A.~Srivastava \emph{et al.}, ``Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,'' \emph{arXiv preprint arXiv:2206.04615}, 2022.
\bibitem{liang}
P.~Liang \emph{et al.}, ``Holistic Evaluation of Language Models,'' \emph{arXiv preprint arXiv:2211.09110}, 2022.
\bibitem{tolman} E. C. Tolman, "Cognitive Maps in Rats and Men," Psychol. Rev., vol. 55, no. 4, pp. 189-208, Jul. 1948.
\bibitem{piaget} J. Piaget and B. Inhelder, "The Child's Conception of Space," Int. J. Psychol., vol. 2, no. 3, pp. 241-242, 1967.
\bibitem{stentz} Path-finding Algorithms: A. Stentz, "Optimal and Efficient Path Planning for Partially Known Environments," in IEEE International Conference on Robotics and Automation, 1994, pp. 3310-3317.
\bibitem{ext4} “LLM Index | Promptmetheus,” Promptmetheus, Mar. 13, 2025. https://promptmetheus.com/resources/llm-index (accessed Mar. 26, 2025).


\bibitem{hendrycks}
D.~Hendrycks \emph{et al.}, ``Measuring Massive Multitask Language Understanding,'' \emph{arXiv preprint arXiv:2009.03300}, 2021.

\bibitem{ahn}
M.~Ahn \emph{et al.}, ``Do As I Can, Not As I Say: Grounding Language in Robotic Affordances,'' \emph{arXiv preprint arXiv:2204.01691}, 2022.

\bibitem{yao}
S.~Yao \emph{et al.}, ``ReAct: Synergizing Reasoning and Acting in Language Models,'' \emph{arXiv preprint arXiv:2210.03629}, 2022.

\bibitem{ivanitskiy}
M.~I. Ivanitskiy \emph{et al.}, ``A configurable library for generating and manipulating maze datasets,'' \emph{arXiv preprint arXiv:2309.10498}, 2023. [Online]. Available: \url{https://arxiv.org/abs/2309.10498}

\end{thebibliography}

\end{document}
